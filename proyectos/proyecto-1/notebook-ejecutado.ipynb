{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROYECTO 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Curso:** Operaciones de Aprendizaje de Máquina\n",
    "\n",
    "**Estudiantes:**\n",
    "- Juan José García\n",
    "- Ruben Dario Hoyos\n",
    "- José Rafael Peña"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importación de librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General modules\n",
    "from pathlib import Path\n",
    "import os\n",
    "import requests\n",
    "from typing import List\n",
    "from dataclasses import dataclass\n",
    "import pandas as pd\n",
    "import pprint as pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sklearn modules\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorflow module\n",
    "import tensorflow as tf\n",
    "\n",
    "# TFX components\n",
    "from tfx.components import CsvExampleGen\n",
    "from tfx.components import ExampleValidator\n",
    "from tfx.components import SchemaGen\n",
    "from tfx.components import StatisticsGen\n",
    "from tfx.v1.components import ImportSchemaGen\n",
    "from tfx.components import Transform\n",
    "from tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext\n",
    "from google.protobuf.json_format import MessageToDict\n",
    "\n",
    "# TFDV modules\n",
    "import tensorflow_data_validation as tfdv\n",
    "from tensorflow_metadata.proto.v0 import schema_pb2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ml_metadata as mlmd\n",
    "from ml_metadata.metadata_store import metadata_store\n",
    "from ml_metadata.proto import metadata_store_pb2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definición de carpetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory of the raw data files\n",
    "data_root = Path('./data/covertype')\n",
    "\n",
    "# Directory of the preprocessed data files\n",
    "data_root_prepro = Path('./data/covertype_prepro')\n",
    "\n",
    "# Path to the raw training data\n",
    "data_filepath = data_root / 'covertype_train.csv'\n",
    "\n",
    "# Ensure the data_root directory exists\n",
    "data_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Ensure the data_root_prepro directory exists\n",
    "data_root_prepro.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Directory of the pipeline metadata store\n",
    "pipeline_root = Path('./pipeline/')\n",
    "\n",
    "# Ensure the pipeline_root directory exists\n",
    "pipeline_root.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data if it doesn't exist\n",
    "if not data_filepath.is_file():\n",
    "    # URL for the dataset\n",
    "    # https://archive.ics.uci.edu/ml/machine-learning-databases/covtype/\n",
    "    url = 'https://docs.google.com/uc?export=download&confirm={{VALUE}}&id=1lVF1BCWLH4eXXV_YOJzjR7xZjj-wAGj9'\n",
    "    \n",
    "    r = requests.get(url, allow_redirects=True, stream=True)\n",
    "    \n",
    "    data_filepath.write_bytes(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pasos proyecto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.1** Carga el dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(data_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3** Selección de características"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataConfig:\n",
    "    target_col: str\n",
    "    non_numeric_cols: List[str]\n",
    "    final_df_path: Path\n",
    "\n",
    "# Creating an instance with specific values\n",
    "config = DataConfig(\n",
    "    target_col=\"Cover_Type\",\n",
    "    non_numeric_cols=list(df.select_dtypes(include=['object']).columns),\n",
    "    final_df_path= data_root_prepro / \"covertype_preprocessed.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La ejecución de la siguiente celda se omite mediante el comando `%%script false --no-raise-error`, ya que contiene la normalización de los datos, un proceso que ya se realizó previamente según lo indicado en el documento:  \n",
    "\n",
    "> **\"Recuerde que, primero, debe preparar las características de entrada y de destino:\"**  \n",
    "\n",
    "Sin embargo, más adelante en el documento se asume que los datos conservan sus valores originales, por lo que la normalización se aplica posteriormente utilizando las herramientas de TFX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "# Drop non-numeric columns\n",
    "df_1 = df.drop(columns=config.non_numeric_cols)\n",
    "\n",
    "# Separate features and label\n",
    "X = df_1.drop(columns=[config.target_col])\n",
    "y = df_1[config.target_col].astype('category')\n",
    "\n",
    "# Scale data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Convert back to DataFrame with original column names\n",
    "X_scaled = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "\n",
    "# Implement f_classif as score function and select the 8 best columns\n",
    "selector = SelectKBest(score_func=f_classif, k=8)\n",
    "selector.fit(X, y)\n",
    "\n",
    "# Create and print a df comparing the column and the result (if its retained or not)\n",
    "selected_columns_df = pd.DataFrame({\n",
    "    'Column': X_scaled.columns,\n",
    "    'Retain': selector.get_support()\n",
    "})\n",
    "selected_columns_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop non-numeric columns\n",
    "df_1 = df.drop(columns=config.non_numeric_cols)\n",
    "\n",
    "# Separate features and label\n",
    "X = df_1.drop(columns=[config.target_col])\n",
    "y = df_1[config.target_col].astype('category')\n",
    "\n",
    "# Implement f_classif as score function and select the 8 best columns\n",
    "selector = SelectKBest(score_func=f_classif, k=8)\n",
    "selector.fit(X, y)\n",
    "\n",
    "# Select the best features using boolean mask\n",
    "X_selected = X.loc[:, selector.get_support()]\n",
    "\n",
    "# Create and print a df comparing the column and the result (if its retained or not)\n",
    "selected_columns_df = pd.DataFrame({\n",
    "    'Column': X.columns,\n",
    "    'Retain': selector.get_support()\n",
    "})\n",
    "selected_columns_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the target column back\n",
    "final_df = X_selected.copy()\n",
    "final_df[config.target_col] = y.values\n",
    "\n",
    "# Save the updated dataframe to CSV\n",
    "final_df.to_csv(config.final_df_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red; font-weight:bold;\">NOTA:</span> Se debe tener cargado el dataset (`final_df`) en memoria, pues posteriormente se debe hacer una división para algunas pruebas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### **4.1** Configurar el contexto interactivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = InteractiveContext(pipeline_root=str(pipeline_root))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### **4.2** Generando ejemplos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate ExampleGen with the input CSV dataset\n",
    "example_gen = CsvExampleGen(input_base=str(data_root_prepro))\n",
    "\n",
    "# Execute the component\n",
    "context.run(example_gen)\n",
    "\n",
    "print(\"CsvExampleGen ok\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### **4.3** Estadísticas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the artifact object\n",
    "artifact = example_gen.outputs['examples'].get()[0]\n",
    "\n",
    "# print split names and uri\n",
    "print(f'split names: {artifact.split_names}')\n",
    "print(f'artifact uri: {artifact.uri}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate StatisticsGen with the ExampleGen ingested dataset\n",
    "statistics_gen = StatisticsGen(\n",
    "    examples=example_gen.outputs['examples'])\n",
    "\n",
    "# Execute the component\n",
    "context.run(statistics_gen)\n",
    "\n",
    "print('StatisticsGen OK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the output statistics\n",
    "context.show(statistics_gen.outputs['statistics'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede observar que, efectivamente, como se menciona en el documento, la columna `cover_type` tiene resaltado en rojo el porcentaje de valores en 0. Esto no es preocupante, ya que es la *target* del problema y una variable categórica.  \n",
    "\n",
    "Por otro lado, se puede mencionar que la *feature* con el coeficiente de variación más alto (`avg/std`) es `Vertical_Distance_To_Hydrology`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### **4.4** Inferir Esquema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate SchemaGen with the StatisticsGen ingested dataset\n",
    "schema_gen = SchemaGen(\n",
    "    statistics=statistics_gen.outputs['statistics'],\n",
    "    )\n",
    "\n",
    "# Run the component\n",
    "context.run(schema_gen)\n",
    "\n",
    "print('SchemaGen OK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the schema\n",
    "context.show(schema_gen.outputs['schema'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### **4.5** Curando Esquema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load schema as tensorflow_metadata.proto.v0.schema_pb2\n",
    "schema_path = schema_gen.outputs['schema'].get()[0].uri + \"/schema.pbtxt\"\n",
    "schema = tfdv.load_schema_text(schema_path)\n",
    "type(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set domains for Hillshade_9am, Hillshade_Noon, Slope, Cover_Type\n",
    "tfdv.set_domain(schema, 'Hillshade_9am', schema_pb2.IntDomain(min=0, max=255))\n",
    "tfdv.set_domain(schema, 'Hillshade_Noon', schema_pb2.IntDomain(min=0, max=255))\n",
    "tfdv.set_domain(schema, 'Slope', schema_pb2.IntDomain(min=0, max=99))\n",
    "domain_cover_type = schema_pb2.StringDomain(value=['0','1', '2', '3', '4', '5', '6'], is_categorical=True)\n",
    "tfdv.set_domain(schema, 'Cover_Type', domain_cover_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se observó que a pesar de agregar un dominio tipo String y especificar que es categorico, el tipo de la variable `Cover_Type` no cambió por lo que manualmente se asigna el tipo de dato correcto y que al mostrar el esquema, el tipo aparezca como `STRING`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually assign Cover_Type feature type to FeatureType.BYTES\n",
    "schema.feature[0].type = schema_pb2.FeatureType.BYTES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En los siguientes bloques se muestra que el esquema ha cambiado solo en memoria. Sin embargo, aún es necesario guardarlo en la metadata de SchemaGen para que los cambios sean persistentes y reconocidos por el pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the updated schema with domains\n",
    "tfdv.display_schema(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the old schema with no domains\n",
    "context.show(schema_gen.outputs['schema'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La siguiente celda garantiza que este quede guardado y ahora sí se podrán observar los cambios por medio del artefacto ya creado schema_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overwrite the file\n",
    "tfdv.write_schema_text(schema, schema_path)\n",
    "\n",
    "# Display updated schema with SchemaGen\n",
    "context.show(schema_gen.outputs['schema'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### **4.6** Entornos de esquema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_serving_csv(df: pd.DataFrame,  path: Path, target: str = config.target_col) -> Path:\n",
    "    train_df, test_df = train_test_split(df, test_size=0.3, random_state=42)\n",
    "    test_df = test_df.drop(columns=[target])\n",
    "    test_df.to_csv(path, index=False)\n",
    "    return str(path)\n",
    "\n",
    "serving_data = generate_serving_csv(final_df, data_root / \"serving_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = tfdv.StatsOptions(schema=schema)\n",
    "serving_stats = tfdv.generate_statistics_from_csv(serving_data, stats_options=options)\n",
    "serving_anomalies = tfdv.validate_statistics(serving_stats, schema)\n",
    "\n",
    "tfdv.display_anomalies(serving_anomalies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All features are by default in both TRAINING and SERVING environments.\n",
    "schema.default_environment.append('training')\n",
    "schema.default_environment.append('serving')\n",
    "\n",
    "# Specify that 'tips' feature is not in SERVING environment.\n",
    "tfdv.get_feature(schema, config.target_col).not_in_environment.append('serving')\n",
    "\n",
    "serving_anomalies_with_env = tfdv.validate_statistics(\n",
    "    serving_stats, schema, environment='serving')\n",
    "\n",
    "tfdv.display_anomalies(serving_anomalies_with_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se procede a guardar el archivo de schema ***tanto en la carpeta de SchemaGen para persitirlo en el pipeline y también se guarda en otro archivo local como se indica en el ejercicio***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overwrite the file in SchemaGen component files and save it in a local file\n",
    "tfdv.write_schema_text(schema, schema_path)\n",
    "tfdv.write_schema_text(schema, data_root.parent / \"schema_entornos.pbtxt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se especifica en el documento se procede a verificar que este schema sí tenga los cambios realizados.\n",
    "\n",
    "> **Como verificación, debe mostrar el esquema que acaba de guardar y verificar que contiene los cambios introducidos**\n",
    "\n",
    "La opción `display_schema` no brinda ninguna información respecto a los entornos, pero mostrando el archivo texto se debe observar los entornos como se presenta en la documentación:\n",
    "```\n",
    "default_environment: \"TRAINING\"\n",
    "default_environment: \"SERVING\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(schema.default_environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### **4.7** Nuevas estadísticas usando el esquema actualizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate SchemaGen with the StatisticsGen ingested dataset\n",
    "path_local_schema = data_root.parent / \"schema_entornos.pbtxt\"\n",
    "schema_gen_2 = ImportSchemaGen(schema_file=str(path_local_schema))\n",
    "\n",
    "# Run the component\n",
    "context.run(schema_gen_2)\n",
    "\n",
    "print('ImportSchemaGen OK')\n",
    "context.show(schema_gen_2.outputs['schema'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_eval_stats = StatisticsGen(\n",
    "      examples=example_gen.outputs['examples'],\n",
    "      schema=schema_gen_2.outputs['schema'],\n",
    "    \n",
    "      )\n",
    "\n",
    "# Execute the component\n",
    "context.run(compute_eval_stats)\n",
    "\n",
    "print('New StatisticsGen OK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the output statistics\n",
    "context.show(compute_eval_stats.outputs['statistics'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se observa un despliegue de estadísticas muy similar al obtenido con el esquema inferido. Esto puede deberse a que CsvExampleGen ya infiere y asigna tipos a las variables, por lo que sería necesario definir estas restricciones de tipo al momento de la lectura."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### **4.8** Comprobar anomalías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate ExampleValidator with the StatisticsGen and ImportSchemaGen ingested data\n",
    "example_validator = ExampleValidator(\n",
    "    statistics=compute_eval_stats.outputs['statistics'],\n",
    "    schema=schema_gen_2.outputs['schema'])\n",
    "\n",
    "# Run the component.\n",
    "context.run(example_validator)\n",
    "\n",
    "print('ExampleValidator OK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the results\n",
    "context.show(example_validator.outputs['anomalies'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso, era previsibile que resalte anomalías, ya que con las nuevas estadísticas no se logra observar el cambio en el tipo de la variable. Se intuye, sin certeza, que esto sucede porque CsvExampleGen ya infiere los tipos, y para que las estadísticas reflejen los nuevos tipos, se debe modificar la lectura."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.9** Ingeniería de características"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the constants module filename\n",
    "constants_module_file = 'constants.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {constants_module_file}\n",
    "# Numerical features that are marked as continuous\n",
    "NUMERIC_FEATURE_KEYS = ['Elevation', 'Hillshade_9am', 'Hillshade_Noon', 'Horizontal_Distance_To_Fire_Points', 'Horizontal_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways', 'Vertical_Distance_To_Hydrology']\n",
    "\n",
    "# Feature that can be grouped into buckets\n",
    "BUCKET_FEATURE_KEYS = ['Slope']\n",
    "\n",
    "# Number of buckets used by tf.transform for encoding each bucket feature.\n",
    "FEATURE_BUCKET_COUNT = {'Slope': 4}\n",
    "\n",
    "# Feature that the model will predict\n",
    "LABEL_KEY = 'Cover_Type'\n",
    "\n",
    "# Utility function for renaming the feature\n",
    "def transformed_name(key):\n",
    "    return key + '_xf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the transform module filename\n",
    "transform_module_file = 'transform.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {transform_module_file}\n",
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft\n",
    "import constants\n",
    "\n",
    "# Desempaquetar los contenidos del módulo de constantes\n",
    "NUMERIC_FEATURE_KEYS = constants.NUMERIC_FEATURE_KEYS\n",
    "BUCKET_FEATURE_KEYS = constants.BUCKET_FEATURE_KEYS\n",
    "FEATURE_BUCKET_COUNT = constants.FEATURE_BUCKET_COUNT\n",
    "LABEL_KEY = constants.LABEL_KEY\n",
    "transformed_name = constants.transformed_name\n",
    "\n",
    "\n",
    "def preprocessing_fn(inputs):\n",
    "    \"\"\"Función de callback de tf.transform para preprocesar entradas.\n",
    "    Args:\n",
    "        inputs: diccionario de características sin transformar.\n",
    "    Returns:\n",
    "        Diccionario de características transformadas.\n",
    "    \"\"\"\n",
    "    outputs = {}\n",
    "\n",
    "    # Escalar características numéricas al rango [0,1]\n",
    "    for key in NUMERIC_FEATURE_KEYS:\n",
    "        outputs[transformed_name(key)] = tft.scale_to_0_1(inputs[key])\n",
    "    \n",
    "    # Crear agrupaciones por rangos en características categorizables\n",
    "    for key in BUCKET_FEATURE_KEYS:\n",
    "        outputs[transformed_name(key)] = tft.bucketize(inputs[key], FEATURE_BUCKET_COUNT[key])\n",
    "\n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore TF warning messages\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "# Instantiate the Transform component\n",
    "transform = Transform(\n",
    "    examples=example_gen.outputs['examples'],\n",
    "    schema=schema_gen_2.outputs['schema'],\n",
    "    module_file=transform_module_file)\n",
    "\n",
    "# Run the component\n",
    "context.run(transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora se procede a obtener ejemplos como se indica en las instrucciones del taller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a helper function to get individual examples\n",
    "def get_records(dataset, num_records):\n",
    "    '''Extracts records from the given dataset.\n",
    "    Args:\n",
    "        dataset (TFRecordDataset): dataset saved by ExampleGen\n",
    "        num_records (int): number of records to preview\n",
    "    '''\n",
    "    \n",
    "    # initialize an empty list\n",
    "    records = []\n",
    "    \n",
    "    # Use the `take()` method to specify how many records to get\n",
    "    for tfrecord in dataset.take(num_records):\n",
    "        \n",
    "        # Get the numpy property of the tensor\n",
    "        serialized_example = tfrecord.numpy()\n",
    "        \n",
    "        # Initialize a `tf.train.Example()` to read the serialized data\n",
    "        example = tf.train.Example()\n",
    "        \n",
    "        # Read the example data (output is a protocol buffer message)\n",
    "        example.ParseFromString(serialized_example)\n",
    "        \n",
    "        # convert the protocol bufffer message to a Python dictionary\n",
    "        example_dict = (MessageToDict(example))\n",
    "        \n",
    "        # append to the records list\n",
    "        records.append(example_dict)\n",
    "        \n",
    "    return records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the URI of the output artifact representing the transformed examples\n",
    "train_uri = os.path.join(transform.outputs['transformed_examples'].get()[0].uri, 'Split-train')\n",
    "\n",
    "# Get the list of files in this directory (all compressed TFRecord files)\n",
    "tfrecord_filenames = [os.path.join(train_uri, name)\n",
    "                      for name in os.listdir(train_uri)]\n",
    "\n",
    "# Create a `TFRecordDataset` to read these files\n",
    "transformed_dataset = tf.data.TFRecordDataset(tfrecord_filenames, compression_type=\"GZIP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede observar al ejecutar la siguiente celda que:\n",
    "- Hay nuevas columnas con \"_xf\" al final\n",
    "- Los valores están 0 y 1\n",
    "- Slope se encuentra entre los valores de 0 a 3, de las tres buckets que definimos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get 3 records from the dataset\n",
    "sample_records_xf = get_records(transformed_dataset, 5)\n",
    "\n",
    "# Print the output\n",
    "pp.pprint(sample_records_xf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5.1** Acceso a artefactos almacenados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para crear un objeto `MetadataStore`, es necesario contar con una configuración de conexión. Existen diferentes formas de establecer esta configuración. Por ejemplo, se puede utilizar una base de datos en memoria mediante SQLite para experimentación rápida y ejecuciones locales:\n",
    "\n",
    "```python\n",
    "connection_config = metadata_store_pb2.ConnectionConfig()\n",
    "connection_config.fake_database.SetInParent()  # Configura una base de datos ficticia en memoria.\n",
    "store = metadata_store.MetadataStore(connection_config)\n",
    "```\n",
    "\n",
    "Otra opción es establecer la conexión a través de un archivo SQLite:\n",
    "```python\n",
    "connection_config = metadata_store_pb2.ConnectionConfig()\n",
    "connection_config.sqlite.filename_uri = 'ruta_a_archivo.sqlite'\n",
    "connection_config.sqlite.connection_mode = 3  # READWRITE_OPENCREATE\n",
    "store = metadata_store.MetadataStore(connection_config)\n",
    "```\n",
    "\n",
    "En nuestro caso, dado que contamos con un archivo de metadatos en formato SQLite, podemos utilizar la segunda opción. Sin embargo, el contexto interactivo `InteractiveContext` que hemos creado ya incluye esta configuración de conexión, lo que nos permite inicializar `MetadataStore` sin necesidad de definirla manualmente. Esto se puede verificar utilizando la función `dir`, donde se observa que el contexto contiene un atributo `metadata_connection_config`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show attributes of the instance context from InteractiveContext\n",
    "dir(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store = metadata_store.MetadataStore(context.metadata_connection_config)\n",
    "print(f\"Hay un total de {len(store.get_artifacts())} artefactos\")\n",
    "print(\"Se inspecciona el artefacto 2 de la lista (posición 1 en la lista): \")\n",
    "store.get_artifacts()[1] # Este método permite ver de manera más exhaustiva todos los artefactos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se despliegan solo los nombres de los tipos de artefactos en nuestro pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in store.get_artifact_types():\n",
    "    print(i.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relación por medio de intuición:\n",
    "- Examples -> Archivo dentro de CsvExampleGen\n",
    "- ExampleStatistics -> Archivo dentro de StatisticsGen\n",
    "- Schema -> Archivo dentro de  SchemaGen, ImportSchemaGen\n",
    "- ExampleAnomalies -> Archivo dentro de ExampleValidator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observación real:\n",
    "\n",
    "Vemos que artefactos de tipo Example también hay en la carpeta de Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in store.get_artifacts_by_type('Examples'):\n",
    "    print(i.uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuacións se exploran los artefactos de tipo Schema como se pide en el documento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in store.get_artifacts_by_type('Schema'):\n",
    "    print(i.uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtener las propiedades de un artefacto en particular como se pide en el documento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in store.get_artifacts_by_type('Schema'):\n",
    "    pp.pprint(i.custom_properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in store.get_artifacts_by_type('Schema'):\n",
    "    pp.pprint(i.custom_properties['producer_component'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se exploran las propiedades de los artefactos de tipo `ExampleStatistics` y se confirma la existencia de propiedades relacionadas con las divisiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in store.get_artifacts_by_type('ExampleStatistics'):\n",
    "    pp.pprint(i.custom_properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5.2** Seguimiento de artefactos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se asume que no se quiere una función que liste los artefactos definiendo un tipo sino instancias usadas en el código como `SchemaGen`, `CsvExampleGen`, `Transform`. Se explica este supuesto ya que `TransfromGraph` es un tipo de artefacto y ya exploramos la función que provee el `MetadataStore` para tener los artefactos por tipo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_artifacts_by_instance(store_obj: mlmd.metadata_store.metadata_store.MetadataStore, instance: str):\n",
    "    return [i for i in store.get_artifacts() if i.custom_properties['producer_component'].string_value == instance]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_artifacts_by_instance(store, 'SchemaGen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_artifacts_by_instance(store, 'Transform')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5.3** Obtener artefactos principales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se exploran los artefactos de tipo schema para obtener el SchemaGen. Se puede observar que este tiene un id 3, y se podría obtener los eventos relacionados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in store.get_artifacts_by_type('Schema'):\n",
    "    print(i.uri, i.id, sep=\" - \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store.get_events_by_artifact_ids([3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede observar que el `execution_id` es 3, y se pueden extraer los eventos relacionados a este id de ejecución"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store.get_events_by_execution_ids([3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí vemos que el evento se relaciona precisamente con el artefacto de `Statistics`, donde se indica un tipo `INPUT`, lo cual debería ser intuitivo, ya que este artefacto de tipo `InferSchemaGen` requiere de estadísticas previas. Además, observamos que finaliza en un artefacto, el cual es el `OUTPUT`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si exploramos el segundo schema, se observan más eventos asociados, esto es coherente debido a que este segundo se uso también en el proceso de transformación por lo que estará asociado a estos procesos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store.get_events_by_artifact_ids([4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entre esos eventos debe estar el que se relaciona con la generación de estadísticas realizadas después de importar el esquema. Se identifica que este está relacionado con el ID de ejecución 5. Así que al explorar los eventos, vemos que se usa el artefacto de `Example` y `ImportSchema` como entradas y da salida un artefacto de `Statistics`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in store.get_events_by_execution_ids([5]):\n",
    "    print(f\"ID: {i.artifact_id} - Key: {i.path.steps[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se explora el `CsvExampleGen` mencionado en el documento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store.get_events_by_artifact_ids([1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se observa que se usa en 3 otros procesos como `INPUT`. Posiblemente para la primera y segunda generación de estadísticas, y en el proceso de transformación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in store.get_events_by_execution_ids([2]):\n",
    "    print(f\"ID: {i.artifact_id} - Key: {i.path.steps[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in store.get_events_by_execution_ids([5]):\n",
    "    print(f\"ID: {i.artifact_id} - Key: {i.path.steps[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in store.get_events_by_execution_ids([7]):\n",
    "    print(f\"ID: {i.artifact_id} - Key: {i.path.steps[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se confirma la hipótesis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
